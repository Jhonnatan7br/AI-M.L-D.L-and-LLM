{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3b3e67",
   "metadata": {},
   "source": [
    "# Exercise 01 Recap ML & Ensemble Learning I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c89362",
   "metadata": {},
   "source": [
    "## Pedagogy\n",
    "\n",
    "This notebook contains both theoretical explanations and executable cells to execute your code.\n",
    "\n",
    "When you see the <span style=\"color:red\">**[TBC]**</span> (To Be Completed) sign, it means that you need to perform an action else besides executing the cells of code that already exist. These actions can be:\n",
    "- Complete the code with proper comments\n",
    "- Respond to a question\n",
    "- Write an analysis\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1834a9",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries used in this notebook here\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c923500",
   "metadata": {},
   "source": [
    "## Part 1. Recap of Classical ML\n",
    "\n",
    "In this part, we will implement the best practice of supervised learning to build a classification model using the decision tree algorithm.\n",
    "\n",
    "We will execute the following steps:\n",
    "- Load the dataset\n",
    "- Split the dataset into training and test set\n",
    "- Hyper-parameter tuning through cross-validation using the training dataset\n",
    "- Evaluation the best model with the best hyper-parameters using the test dataset\n",
    "- Inference with unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba0dea",
   "metadata": {},
   "source": [
    "### 1.1. Load dataset\n",
    "\n",
    "We will use a toy dataset, the Iris plants dataset, provided by `scikit-learn` here.\n",
    "\n",
    "There are four feature variables and one target variables in this dataset, which are:\n",
    "- Feature\n",
    "    - Sepal length in cm\n",
    "    - Sepal width in cm\n",
    "    - Petal length in cm\n",
    "    - Petal width in cm\n",
    "- Target\n",
    "    - Class of iris\n",
    "        - Iris-Setosa\n",
    "        - Iris-Versicolour\n",
    "        - Iris-Virginica\n",
    "        \n",
    "There are 150 labeled examples in the dataset.\n",
    "\n",
    "Use `sklearn.datasets.load_iris()` to get this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "feature_df, target_df = datasets.load_iris(\n",
    "    return_X_y = True, # If True, returns (data.data, data.target) instead of a Bunch object.\n",
    "    as_frame = True # If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64994ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first five rows of the features\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223788c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the unique values of the target variable\n",
    "target_df.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60f641",
   "metadata": {},
   "source": [
    "### 1.2. Train test split\n",
    "\n",
    "We will split the whole dataset into two parts: the training and test dataset.\n",
    "- 70% for training\n",
    "- 30% for test\n",
    "\n",
    "Use `sklearn.model_selection.train_test_split()` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_df.values, # call `.values` to convert the feature from pd.DataFrame to np.array\n",
    "    target_df.values, # ca;; `.values` to convert the target from pd.Series to np.array\n",
    "    train_size = 0.7, # 70% for training, 30% for test\n",
    "    random_state = 0 # controls the shuffling, set to zero for reproduciblillity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc38cf6",
   "metadata": {},
   "source": [
    "### 1.3. Hyper-parameter tuning through cross-validation\n",
    "\n",
    "We will perform hyper-parameter tuning through cross-validation using `sklearn.model_selection.GridSearchCV()` on the training dataset.\n",
    "- First, define the potential values of hyper-parameters for search\n",
    "- Call `sklearn.model_selection.GridSearchCv()` to perform an exhausive search\n",
    "    - Use weighted average of F1-Score as the evaluatoin metric\n",
    "    - Set the number of folds to 5 for the cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyper-parameters to search\n",
    "param_dict = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b404856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter tuning through cross-validation\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = DecisionTreeClassifier(),\n",
    "    param_grid = param_dict,\n",
    "    scoring = 'f1_weighted',\n",
    "    refit = True,\n",
    "    cv = 5,\n",
    "    verbose = 1,\n",
    "    n_jobs = -1\n",
    ")\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the best hyper-parameters and the best score\n",
    "print('Best hyper-parameters:', grid_clf.best_params_)\n",
    "print('Best score:', grid_clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad99e714",
   "metadata": {},
   "source": [
    "### 1.4. Evaluation using the test dataset\n",
    "\n",
    "After finding the best combination of hyper-parameter values, we can use the test dataset to evaluate the performance of the best model.\n",
    "\n",
    "If the parameter `refit` was set to `True`, the `GridSearchCV()` has already refitted the model using the best hyper-parameters on the whole training dataset. There is no need to manually refit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict categories for test dataset\n",
    "y_pred = grid_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain classification metrics using `classification_report`\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282cddd",
   "metadata": {},
   "source": [
    "### 1.5. Inference with unseen data\n",
    "\n",
    "After evaluation, if we are satisfied with the performance of the model, we can then use the learned model to make predictions on unseen data, so called inference.\n",
    "- Predict the category directly\n",
    "- Predict the probability of belonging to a specific category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference with unseen data\n",
    "X_unseen = np.array([5.2, 3.1, 1.3, 0.2])\n",
    "print(\"The predicted category of the unseen data:\", grid_clf.predict(X_unseen.reshape(-1, 4)))\n",
    "print(\"The predicted probabilities:\", grid_clf.predict_proba(X_unseen.reshape(-1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e664abf8",
   "metadata": {},
   "source": [
    "## Part 2. The bagging method - Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5136da50",
   "metadata": {},
   "source": [
    "In this part, we will use the random forests algorith, a bagging method for ensemble learning, to build a classifier with the same dataset.\n",
    "\n",
    "Random forests is a meta model that fits a number of decision tree classifiers on various sub-samples of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba40606",
   "metadata": {},
   "source": [
    "### 2.1. Train the random forest using the training dataset\n",
    "\n",
    "To test whether random forest can improve prediction accuracy compared to a single decision tree model, we set the hyperparameters of each decision tree in the random forest to be the same as the decision tree model after hyperparameter optimization in the Part 1.\n",
    "\n",
    "The only difference is that, random forest will fit 10 decision trees using 10 different subsets of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d14a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the random forest classifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators = 10, # set the number of decision trees to 10\n",
    "    criterion = grid_clf.best_params_['criterion'],\n",
    "    max_depth = grid_clf.best_params_['max_depth'],\n",
    "    max_features = grid_clf.best_params_['max_features'],\n",
    "    min_impurity_decrease = grid_clf.best_params_['min_impurity_decrease'],\n",
    "    min_samples_leaf = grid_clf.best_params_['min_samples_leaf'],\n",
    "    min_samples_split = grid_clf.best_params_['min_samples_split'],\n",
    "    n_jobs = -1, # -1 means using all available processors for parallelized computation\n",
    "    random_state = 0 # set random state to 0 for reproduciblity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training dataset\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8194d6",
   "metadata": {},
   "source": [
    "### 2.2. Evaluate the random forest using the test dataset\n",
    "\n",
    "After training, we can using the test dataset to evaluate the performance of the random forest and compare it with the performance of the single decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b30c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict categories for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain classification metrics using `classification_report`\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d704a",
   "metadata": {},
   "source": [
    "Normally, the performance of random forest should be better than, at least not worse than, the single decision tree classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d55cbd",
   "metadata": {},
   "source": [
    "### 2.3. Obtain feature importances\n",
    "\n",
    "Ranomd forests provide the importances of different features for the model to make the correct predictions.\n",
    "\n",
    "We can call the attribute `feature_importances_` to get this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain feature importances\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the boxplots of the 4 features for the 3 different classes\n",
    "# merge the feature and target into one DataFrame\n",
    "df = feature_df\n",
    "df['class'] = target_df\n",
    "# create the figure\n",
    "df.boxplot(\n",
    "    column = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'],\n",
    "    by = 'class',\n",
    "    grid = False,\n",
    "    figsize = (8, 5),\n",
    "    layout = (1, 4)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6b7ce",
   "metadata": {},
   "source": [
    "We can see that for different iris classes:\n",
    "- Petal length and width are distributed in different intervals, so the importances of these two features are higher.\n",
    "- The distribution intervals of sepal length overlap slightly, so the importance is low.\n",
    "- The distribution intervals of sepal width overlap strongly, so the importance is close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c458e8",
   "metadata": {},
   "source": [
    "### 2.4. Inference with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the category and probabilities for the unseen data\n",
    "print(\"The predicted category of the unseen data:\", clf.predict(X_unseen.reshape(-1, 4)))\n",
    "print(\"The predicted probabilities:\", clf.predict_proba(X_unseen.reshape(-1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4d031",
   "metadata": {},
   "source": [
    "## Part 3. Hands-on exercise\n",
    "\n",
    "In this exercise, you are required to build a regression model using the random forests algorithm.\n",
    "\n",
    "The problem to be solved is predicting the price of flights.\n",
    "\n",
    "Please download the flight price dataset from Learn.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Load and explore the dataset\n",
    "- Preprocess the dataset\n",
    "- Build and evaluate a regression model using random forests with default hyper-parameters\n",
    "- Hyper-parameter tuning through cross-validation for random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f2a43",
   "metadata": {},
   "source": [
    "### Task 1. Load and explore the dataset\n",
    "\n",
    "Dataset contains information about flight booking options from the website Easemytrip for flight travel between India's top 6 metro cities.\n",
    "\n",
    "There are 300,261 datapoints and 11 features in the cleaned dataset.\n",
    "\n",
    "More details can be found in this [link](https://www.kaggle.com/datasets/shubhambathwal/flight-price-prediction?resource=download#).\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Load the dataset\n",
    "- Obtain the general information of the dataset\n",
    "- Obtain the number of unique values of each feature in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec022d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# load the dataset\n",
    "# hint: pandas.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# obtain the general information of the dataset\n",
    "# hint: pandas.DataFrame.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2086f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# obtain the number of unique values of each feature in the dataset\n",
    "# hint: pandas.DataFrame.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4b66d",
   "metadata": {},
   "source": [
    "### Task 2. Preprocess the dataset\n",
    "\n",
    "You may find some features in the dataset is categorical. Before training, you need to convert these categorical features into numerical variables.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Encode categorical features\n",
    "- Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Encode categorical features\n",
    "# hint: sklearn.preprocessing.LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Train test split\n",
    "# hint: sklearn.model_selection.train_test_split()\n",
    "# hint: first divide the encoded dataset into features and target, then perform train test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aaa305",
   "metadata": {},
   "source": [
    "### Task 3. Build and evaluate a regression model using random forests with default hyper-parameters\n",
    "\n",
    "You need to train a regression model using random forests algorithms with default hyper-parameters and evaluate its performance.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Train the regression model\n",
    "- Evaluate the performance on test dataset\n",
    "\n",
    "Warning: The difficulty with this task lies in the size of the data set. If your computer doesn't have enough memory to complete the task, you need to find workarounds.\n",
    "\n",
    "- You may reduce the size of training set, but be careful of the size of test set. A overly large test set may cause failure when you use the learned model to make predictions on it.\n",
    "- You may adjust the hyper-parameters of random forests to reduce the complexity of model, which will reduce the memory required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# train the regression model\n",
    "# hint: sklearn.ensemble.RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df51b6ce-eb29-4c56-a166-e5fd81ea1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# evaluate the performance on test dataset\n",
    "# hint: you may use RMSE and r2 score\n",
    "# hint: you may also create a plot to visualize the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d644a27",
   "metadata": {},
   "source": [
    "### Task 4. Hyper-parameter tuning through cross-validation for random forests\n",
    "\n",
    "You need to perform hyper-parameter tuning through cross-validation for random forests.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Hyper-parameter tuning through cross-validation\n",
    "- Evaluate the performance of the final model on test dataset\n",
    "- Get feature importances from the final model\n",
    "\n",
    "Warning: The difficulty with this task lies in the size of the data set. If your computer doesn't have enough memory to complete the task, you need to find workarounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Hyper-parameter tuning through cross-validation\n",
    "# hint: sklearn.model_selection.GridSearchCV()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# evaluate the performance of the final model on test dataset\n",
    "# hint: you may use RMSE and r2 score\n",
    "# hint: you may also create a plot to visualize the predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Get feature importances from the final model\n",
    "# hint: you may first obtain the best estimator from GridSearchCV\n",
    "# hint: then get feature importances from the best estimator\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
