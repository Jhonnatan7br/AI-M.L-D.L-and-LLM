{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1f5866-541e-4a32-bba4-715be0e35b1c",
   "metadata": {},
   "source": [
    "# Exercise 04 ANN for Regression - Instruction\n",
    "\n",
    "## Pedagogy\n",
    "\n",
    "This notebook serves as an instruction for implementing an artificial neural network using Pytorch to develop a linear regression model.\n",
    "\n",
    "Please use this notebook as a reference and guide to complete the assignment.\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7f75c-def3-40a2-bcf7-c5019b0eb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf0f5b-abda-4018-abbe-a44461b82476",
   "metadata": {},
   "source": [
    "## Part 1. Implement an ANN as a linear regression model\n",
    "\n",
    "### Step 1. Build the data pipeline\n",
    "\n",
    "Training an artificial neural network is an iterative process.\n",
    "\n",
    "We need to feed a batch of training samples to the network at each iteration, make predictions, compute loss and gradients, and update learning parameters accordingly.\n",
    "\n",
    "In practice, the whole training set is divided into several batches. The number of batches depends on the batch size and the size of training set. Once all batches are fed to the network, we say an epoch is completed.\n",
    "\n",
    "Training a network for one epoch is usually not enough. Therefore, we need to go through the training sets repeatedly for many epoches to get a well-fitted model.\n",
    "\n",
    "That's why we need to build a data pipeline that can continuously and iteratively feed batches of taining samples to the network.\n",
    "\n",
    "`PyTorch` has two high-level APIs to work with data:\n",
    "- `torch.utils.data.Dataset`\n",
    "- `torch.utils.data.DataLoader`\n",
    "\n",
    "`torch.utils.data.Dataset` stores the samples and their corresponding lables.\n",
    "`torch.utils.data.DataLoader` wraps an iterable object around the `torch.utils.data.Dataset`.\n",
    "\n",
    "For this notebook, we will use a toy dataset, the diabetes dataset, from `sklearn`. You can find more details about this dataset at this [link](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html).\n",
    "\n",
    "As the diabetes dataset is a tabular dataset that consists of numerical feature and target variables. We can use `torch.utils.data.TensorDataset`, a custom class that inherits from the `torch.utils.data.Dataset` class.\n",
    "\n",
    "In this step, we need to:\n",
    "1. Load the diabetes dataset\n",
    "2. Divide the dataset into the training and test set\n",
    "3. Create a `TensorDataset` instance to store the training/test set\n",
    "4. Create a `DataLoader` instance to wrap the training/test set as an iterable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ea59a-e920-4a85-a459-9ea58f111cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load diabetes dataset\n",
    "feature, label = datasets.load_diabetes(\n",
    "    return_X_y = True,\n",
    "    as_frame = False, # get data as numpy array\n",
    "    scaled = True # ANN requires to scale the input features\n",
    ")\n",
    "# train test split\n",
    "train_feature, test_feature, train_label, test_label = train_test_split(\n",
    "    feature,\n",
    "    label,\n",
    "    train_size = 0.7,\n",
    "    shuffle = True,\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66063f-dfac-4dbc-a5dd-e744301e386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test dataset\n",
    "# specify the type of data stored in the tensors to avoid incompatiblity\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(train_feature, dtype = torch.float32),\n",
    "    torch.tensor(train_label, dtype = torch.float32)\n",
    ")\n",
    "test_ds = TensorDataset(\n",
    "    torch.tensor(test_feature, dtype = torch.float32),\n",
    "    torch.tensor(test_label, dtype = torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def3f95-7cc6-4832-8e59-521cd65f6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test data loaders\n",
    "batch_size = 16 # usually set to 2 to the nth power\n",
    "train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle = False)\n",
    "# shuffle = True means the data is reshuffled at every epoch\n",
    "# recommend to reshuffle training data\n",
    "# don't reshuffle test data since test data will be fed to network only once\n",
    "# we may also need to keep the order of test samples in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637021b6-780b-42d5-90ee-b1ab5022851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a minibatch from the data loader and print shape of feature and label\n",
    "for (X, y) in train_dl:\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a78997-6bff-447e-b7a1-2ddbb9e6e48b",
   "metadata": {},
   "source": [
    "We can see the feature batch has two dimensions:\n",
    "- 1st dimension is the batch size\n",
    "- 2nd dimension is the number of features\n",
    "\n",
    "The label batch has only one dimention, which is the batch size. This is because we only have one target variable as the label to predict. If there are multiple target variables to predice using the single network, the label batch will also have two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbd0ac-38c0-40c9-ab31-2041c21bf8a3",
   "metadata": {},
   "source": [
    "### Step 2. Create the artificial neural network\n",
    "\n",
    "To define a neural network in `PyTorch`, we create a class that inherits from `torch.nn.Module`.\n",
    "\n",
    "We need to define the layers of the network in the `__init__()` funcion and specify how data will pass through the network in the `forward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3dc2c-5314-4461-a0c3-aa7a126a41bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features, n_labels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(n_features, n_labels)\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255426a-3169-42e0-962c-e5e4531c1488",
   "metadata": {},
   "source": [
    "We can see we defined a simple network with only one layer. The input size of this layer is the number of feature variables, the output size is the number of target variables. We didn't add any layer to represent activation function, which indicates the activation function is linear.\n",
    "\n",
    "This is how we use artificial neural network as a linear regression model.\n",
    "\n",
    "Read the `PyTorch` documentation:\n",
    "- `torch.nn.Linear()` at this [link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "Create a class to define the ANN is not enough. We also need to create an instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fb165-8416-4c4a-8505-a88b746e73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the neural network\n",
    "model = NeuralNetwork(\n",
    "    n_features = feature.shape[1],\n",
    "    n_labels = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bfed9-b1ae-4c0e-b144-e13306377c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print network structure and learning parameters\n",
    "print(f\"Model structure:\\n{model}\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f73d0-f63e-45a7-a072-dc089b331333",
   "metadata": {},
   "source": [
    "### Step 3. Training by gradient descent\n",
    "\n",
    "To train a neural network, we need:\n",
    "- A data loader of the training set (batch size)\n",
    "- A loss function\n",
    "- An optimizer (learning rate)\n",
    "- The number of epoches to train\n",
    "\n",
    "We can define a `train()` function that takes the above elements as parameters and performs the training process.\n",
    "\n",
    "In a single training loop, the neural network makes predictions on the training samples (fed to it in batches), and backpropagates the prediction error to adjust the learning parameters. It's also good to record the changes in loss for further analysis and adjustment.\n",
    "\n",
    "Let's defind a `train()` function to implement this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2239d86-1700-43d0-b7ab-f57dfbf91c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function\n",
    "def train(dataloader, model, loss_fn, optimizer, epochs):\n",
    "    batch_loss_history = [] # for recording the average loss of a batch\n",
    "    epoch_loss_history = [] # for recording the average loss of an epoch\n",
    "    model.train() # set the model in training mode\n",
    "    for epoch in range(epochs): # iterate pre-defined number of epoches\n",
    "        epoch_loss = 0.0 # initial epoch loss is set to zero\n",
    "        for (X, y) in dataloader: # get a batch of training samples\n",
    "            pred = model(X).squeeze() # make predictions, squeeze() reduce `pred` to 1D tensor\n",
    "            batch_loss = loss_fn(pred, y) # compute the current batch loss\n",
    "            batch_loss.backward() # compute gradients by backpropagation\n",
    "            optimizer.step() # update learning parameters according to gradients\n",
    "            optimizer.zero_grad() # reset the gradients to zero\n",
    "            batch_loss_history.append(batch_loss.item()) # record current batch loss\n",
    "            epoch_loss += batch_loss.item() # accumulate batch losses for compute epoch loss\n",
    "        epoch_loss /= len(dataloader) # compute current epoch loss\n",
    "        epoch_loss_history.append(epoch_loss) # record current epoch loss\n",
    "        print(f\"Epoch {epoch + 1}: train loss = {epoch_loss}\") # print log\n",
    "    return batch_loss_history, epoch_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449ee81-b2f5-4fb6-9b75-b2fa3078dcd2",
   "metadata": {},
   "source": [
    "After define the `train()` function, we need to specify the loss function, learning rate, optimizer, and number of epoches before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156b3d8-0ba4-4026-8675-277f294fe72f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the training hyper-parameters\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d75b30-4a9d-4bd3-8e0d-25596ce211c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "batch_loss_history, epoch_loss_history = train(train_dl, model, loss_fn, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ba4b2-9984-41c4-bff6-392661d74fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train loss history\n",
    "plt.figure()\n",
    "batch = np.arange(len(batch_loss_history))\n",
    "epoch = np.arange(len(epoch_loss_history))\n",
    "batches_per_epoch = (int(len(train_feature) / batch_size + 1))\n",
    "plt.plot(batch, batch_loss_history, '-', label = 'batch loss')\n",
    "plt.plot(epoch * batches_per_epoch, epoch_loss_history, '-', label = 'epoch loss')\n",
    "plt.title('train loss history')\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57018150-2e38-49c7-9d89-00e66dc11602",
   "metadata": {},
   "source": [
    "The loss is gradually reduced by mini-batch gradient descent.\n",
    "\n",
    "Due to the randomness in selecting each batch, we can find that although the batch loss shows a decreasing trend, it still fluctuates significantly.\n",
    "\n",
    "However, we can't find such significant fluctuation of the epoch loss. This also proves the effectiveness of mini-batch gradient descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3782e4-90b2-4752-b017-e6e1fadc3b07",
   "metadata": {},
   "source": [
    "### Step 4. Save and load a trained model\n",
    "\n",
    "Training a neural network oftern takes a lot of time. We don't want to retrain every time we use it.\n",
    "\n",
    "A better approach is to save the trained model after training is completed and load it again for subsequent use.\n",
    "\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161a8c4-4bfd-4d6b-8cd8-a142adecf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "file_name = 'model.pth'\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print('Saved PyTorch Model State to '+ file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b43623-5d45-44ea-8758-5451ce165499",
   "metadata": {},
   "source": [
    "The process for loading a model includes re-creating the model structure and loading the state dictionary (containing the model parameters) into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f4242-f5bd-4876-b4a5-74dae17ae91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = NeuralNetwork(\n",
    "    n_features = feature.shape[1],\n",
    "    n_labels = 1\n",
    ")\n",
    "model.load_state_dict(torch.load(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fb37e-b814-41b5-8909-e153a71c545f",
   "metadata": {},
   "source": [
    "The `<All keys matched successfully>` means we successfully loaded the trained model to the newly created neural network.\n",
    "\n",
    "If the structure of the newly created network is different from the structure of the trained model, we will get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d00cc-dc13-4974-b704-fcb2752de7b6",
   "metadata": {},
   "source": [
    "### Step 5. Make predictions and evaluation\n",
    "\n",
    "The loss value on the training set can only represent the fitness of the neural network to the training data. We also need to ensure the trained network generalize well on unseen data.\n",
    "\n",
    "Therefore, we need to make predictions on the test set and evaluation the generalization ability.\n",
    "\n",
    "We can feed the entire test set to the network to make predictions. Or we can do it in batch. It's better to do it in batch since we may deal with very big dataset, feeding the entire test set to the network may exceed the available memory of your computer.\n",
    "\n",
    "To do that in batch, we can define a `test()` function, which is similar to the `train` function but much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8c150-41bb-4b93-bf26-c23d4cd159a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to make predictions on test dataset and evaluate the performance\n",
    "def test(dataloader, model, loss_fn):\n",
    "    batch_pred_list = [] # for recording batch predictions\n",
    "    model.eval() # set the model in evaluation mode\n",
    "    with torch.no_grad(): # disable automatic gradient computing\n",
    "        loss = 0.0 # set initial test loss to zero\n",
    "        for (X, y) in dataloader: # get a batch from test samples\n",
    "            batch_pred = model(X).squeeze() # make predictions, squeeze() reduce `batch_pred` to 1D tensor\n",
    "            batch_loss = loss_fn(batch_pred, y) # compute current batch loss\n",
    "            loss += batch_loss.item() # accumulate batch losses for compute test loss\n",
    "            batch_pred_list.append(batch_pred) # record predictions on current batch\n",
    "        loss /= len(dataloader) # compute test loss\n",
    "        pred = np.concatenate(batch_pred_list) # reform the predictions as a numpy 1D array\n",
    "        print(f\"test loss = {loss}\") # print log\n",
    "    return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67cb23-87d3-475e-9e3f-0fa30197bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on test set and evaluate the performance\n",
    "test_pred, test_loss = test(test_dl, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380b242-593b-4e62-bb1f-d26a42eee242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction results of the test dataset\n",
    "plt.figure()\n",
    "plt.plot(test_label, test_pred, '.')\n",
    "plt.plot([min(test_label), max(test_label)], [min(test_label), max(test_label)], '-')\n",
    "plt.xlabel('target value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b761f-f807-45da-833d-fe8d04c966ef",
   "metadata": {},
   "source": [
    "## Part 2. Implement a multi-layer ANN for regression\n",
    "\n",
    "We already implement our 1st one-layer ANN as a linear regression model.\n",
    "- There are only one input layer and one output layer in the network\n",
    "- The number of input neurons in the input layer is equal to the number of input features\n",
    "- The number of output neurons is 1, which is equal to the number of target variables to predict\n",
    "- The activation function is somehow ignored, which indicates the output neuron adopts the linear activation function: $f(z)=z$.\n",
    "\n",
    "Therefore, the output of this one-layer ANN is $\\hat{y}=w_1x_1+w_2x_2+...+w_nx_n+b$, the same as a linear regression model.\n",
    "\n",
    "However, we already knew that such model might not be complex enough to learn the hidden patterns in the dataset and to solve complex regression problems.\n",
    "\n",
    "### Option 1. Increase the capacity of the ANN\n",
    "\n",
    "In the context of ANN, the capacity of an one-layer ANN might be too small for a complex regression model. We can increase this capacity by constructing a multi-layer ANN, which stacks multiple layers in sequence to form a deeper ANN with larger capacity.\n",
    "\n",
    "To do that, we need to be careful of:\n",
    "- The input size of the first layer is equal to the number of features\n",
    "- The input size of a layer is equal to the output size of the previous layer\n",
    "- If we have multiple layers stacked together, we can create an ordered container of layers using `torch.nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd80cf0-834d-4dce-9f82-42a651fdc055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features, n_labels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 16),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Linear(4, n_labels)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fef123-cf59-4937-941c-c7573372f21e",
   "metadata": {},
   "source": [
    "We defined a 4-layer network with linear activation functions.\n",
    "\n",
    "The `forward()` function remains the same. This is becaues we embedded multiple `nn.Linear()` layers into one `nn.Sequential()` container, which can be used as a single layer in forward propagation.\n",
    "\n",
    "Read the `PyTorch` documentation:\n",
    "- `torch.nn.Sequential()` at this [link](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a823f-8ecf-4217-ae93-c4a4dc052223",
   "metadata": {},
   "source": [
    "This is how we can increase the capacity of the ANN by adding more layers in the network.\n",
    "\n",
    "<span style=\"color:red\">Now use this multi-layer ANN to build a regression model using the same dataset and steps in Part 1.</span>\n",
    "- Can you increase model's performance comparing with the one-layer ANN?\n",
    "- Try to change the batch size, learning rate, number of layers and neurons to achieve a good performance without increasing the capacity too much.\n",
    "- If you get a loss value of `nan`, this is because the loss value exceeds the range that can be expressed by a `float32` object.\n",
    "    - This is what we called gradient explosion\n",
    "    - You can try to decrease the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7af421-9587-43a9-8a21-382cc84225e5",
   "metadata": {},
   "source": [
    "### Option 2. Add non-linearity in ANN\n",
    "\n",
    "No matter how many hidden layers and neurons we add to the network, the output is sill the linear combination of the input features, which can only represent the linear relationship between the feature vairables and the target variable.\n",
    "\n",
    "To add non-linearity in the network, we can change the activation function from a linear one to a non-linear one. To do that:\n",
    "- We can add activation function as a separate layer following the corresponding layer, for example, a `torch.nn.ReLU()` follows a `torch.nn.Linear()`\n",
    "- Or we can embed the activation function into the `forward()` function, we will not show this way in this notebook.\n",
    "\n",
    "The most popular non-linear activation functions are:\n",
    "- ReLU, `torch.nn.ReLU()`, see the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU).\n",
    "- Tanh, `torch.nn.Tanh()`, see the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh).\n",
    "- Sigmoid, `torch.nn.Sigmoid()`, see the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid).\n",
    "\n",
    "You can find all the pre-defined types of layers and activation function in PyTorch at this [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n",
    "\n",
    "These pre-defined layers and activation functions give us the great flexibility in the structure of the ANN.\n",
    "\n",
    "Now let's introduce the non-linearity to the ANN by adding non-linear activation function layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2741c-9c10-422b-ac81-57b4206b85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features, n_labels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, n_labels)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcbcae-fdc2-4922-9ee8-a4fc8c7c2888",
   "metadata": {},
   "source": [
    "We add the ReLU activation function for the three hidden layers in the network.\n",
    "\n",
    "Note that, we usually don't add any non-linear activation function for the output layer.\n",
    "\n",
    "<span style=\"color:red\">Now use this non-linear ANN to build a regression model using the same dataset and steps in Part 1.</span>\n",
    "- Can you increase model's performance comparing with the linear ANN?\n",
    "- Try to change the activation functions and also other hyper-parameters to achieve a good performance.\n",
    "    - If your network always predict the average value of the target variables, it means the capacity of the network is not enough to capture the non-linear relationship, try to increase the capacity.\n",
    "    - If the train loss is much lower than the test loss, it means your network is over-fitted. Try to decrease the capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29637f73-334e-4392-9b77-375034cc4cc5",
   "metadata": {},
   "source": [
    "We often refer to the training process of deep learning as alchemy because there are so many hyperparameters that we can adjust.\n",
    "\n",
    "We will learn more techniques in later lessons to ensure that our models are not overfitting or underfitting. But experience is still extremely important.\n",
    "\n",
    "Try as many different combinations of training parameters as possible, gain experience by observing the problems you encounter and solving them by adjusting the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
