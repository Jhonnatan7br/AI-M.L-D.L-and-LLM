{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24249bb-a444-43c9-8602-1303cbeb2ad6",
   "metadata": {},
   "source": [
    "# Exercise 05 ANN for Multi-class Classificatoin - Instruction\n",
    "\n",
    "## Pedagogy\n",
    "\n",
    "This notebook serves as an instruction for implementing ANNs using Pytorch to develop multi-class classification models.\n",
    "\n",
    "Please use this notebook as a reference and guide to complete the assignment.\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cbb20-e752-49a5-9c08-896cf1a8723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23adfef-543c-4772-b263-4eebc71cc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18187d3f-4cd8-4bb3-8f61-3f098d738f07",
   "metadata": {},
   "source": [
    "## Part 1. ANN for Multi-class Classification\n",
    "\n",
    "### Step 1. Build the data pipeline\n",
    "\n",
    "We will use a toy dataset, the irir dataset, from `sklearn`. You can find more detials about this dataset at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris).\n",
    "\n",
    "As the iris dataset is a tabular dataset that consists of numerical feature variables and a categorical target variable. We can use `torch.utils.data.TensorDataset`, a custom class that inherits from the `torch.utils.data.Dataset` class.\n",
    "\n",
    "In this step, we need to:\n",
    "1. Use `sklearn.datasets.load_iris()` to load the dataset\n",
    "2. Perform feature scaling for the numerical feature variables\n",
    "3. <span style=\"color:red\">Perform one-hot encoding for the categorical target variable</span>\n",
    "4. Divide the dataset into the training, validation, and test set\n",
    "5. Create `TensorDataset` instances to store the training, validation, and test set\n",
    "    - <span style=\"color:red\">For numerical features, set `dtype = torch.float32`</span>\n",
    "    - <span style=\"color:red\">For categorical labels, set `dtype = torch.long`</span>\n",
    "7. Create `DataLoader` instances to wrap the training, validation, and test set as iterable objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a383f-6965-4bad-b47e-2ae6d5186e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "feature, label = datasets.load_iris(\n",
    "    return_X_y = True,\n",
    "    as_frame = False, # get data as numpy array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ebe72-ff12-40d2-b54a-629179ca0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_feature = scaler.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f040b-7c24-4b6d-b769-72e3a09ba07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoding for label\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "encoded_label = encoder.fit_transform(label.reshape(-1, 1))\n",
    "encoded_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a1d47-4bb2-41c9-b875-2e700705a0e8",
   "metadata": {},
   "source": [
    "We can see the `encoded_label` has three columns, each corresponds to a class.\n",
    "- If the instance belongs to the 1st class, then the value in the 1st columns is 1, and the values in the other columns are zero\n",
    "- There can only be one `1` in a row, tha't why we call it one-hot\n",
    "- Find more details in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513a63-9b8a-49b3-ba45-2cb5087647d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_val_feature, test_feature, train_val_label, test_label = train_test_split(\n",
    "    scaled_feature, label, test_size = 0.2, random_state = 0\n",
    ")\n",
    "train_feature, val_feature, train_label, val_label = train_test_split(\n",
    "    train_val_feature, train_val_label, test_size = 0.25, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0d77d-7220-4fdf-83db-efd6b639d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train, validatoin, and test dataset\n",
    "# specify the type of data stored in the tensors to avoid incompatiblity\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(train_feature, dtype = torch.float32),\n",
    "    torch.tensor(train_label, dtype = torch.long)\n",
    ")\n",
    "val_ds = TensorDataset(\n",
    "    torch.tensor(val_feature, dtype = torch.float32),\n",
    "    torch.tensor(val_label, dtype = torch.long)\n",
    ")\n",
    "test_ds = TensorDataset(\n",
    "    torch.tensor(test_feature, dtype = torch.float32),\n",
    "    torch.tensor(test_label, dtype = torch.long)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3bbfc-46b5-4cc7-8de0-dd69b72b2e3a",
   "metadata": {},
   "source": [
    "For numerical featuers, we can set `dtype = torch.float32`.\n",
    "\n",
    "For categorical labels, we need to set `dtype = torch.long`.\n",
    "\n",
    "Find all available data types of `PyTorch` in the [documentation](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315dddfa-f1cf-4b15-b6ef-2ce820879c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test data loaders\n",
    "batch_size = 8 # usually set to 2 to the nth power\n",
    "train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "val_dl = DataLoader(val_ds, batch_size = batch_size, shuffle = False)\n",
    "test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481264b0-bea3-47f9-8e63-0f4dece30779",
   "metadata": {},
   "source": [
    "### Step 2. Create the artificial neural network\n",
    "\n",
    "To define a neural network in `PyTorch`, we create a class that inherits from `torch.nn.Module`.\n",
    "\n",
    "We create a multi-layer ANN here for the multi-class problem:\n",
    "- We add <span style=\"color:red\">batch normalization</span> layers and <span style=\"color:red\">dropout</span> layers using:\n",
    "    - `nn.BatchNorm1d()`\n",
    "    - `nn.Dropout()`\n",
    "- The number of output neurons is equal of the number of classes\n",
    "- The activation functions of output neurons are linear\n",
    "    - To get <span style=\"color:red\">logits</span> as the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c9f6f-aa08-4058-9121-e0093f64c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom neural network class\n",
    "class MultiClassificationNet(nn.Module):\n",
    "    def __init__(self, n_features, n_labels):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 16),\n",
    "            nn.BatchNorm1d(16), # batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5), # dropout with 50% probability\n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8), # batch normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = 0.5), # dropout with 50% probability\n",
    "            nn.Linear(8, n_labels)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89d168-6a36-429b-a2c1-8e3b3ac3d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the neural network\n",
    "model = MultiClassificationNet(\n",
    "    n_features = feature.shape[1],\n",
    "    n_labels = encoded_label.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815764d-70ec-4558-9965-43a970bd8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print network structure and learning parameters\n",
    "print(f\"Model structure:\\n{model}\\n\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1e980-421a-41b5-8239-6bde1b1720a8",
   "metadata": {},
   "source": [
    "We can see, apart from the linear layers, the batch normalization layers also have learning parameters.\n",
    "- These learning parameters are also optimized during the training process\n",
    "- The network may learn to deactivate the batch normalization, then:\n",
    "    -  the learned weight is equal to the original standard deviation of the corresponding feature before batch normalization\n",
    "    -  the learned bias is equal to the original mean of the corresponding feature before batch normalization\n",
    "    -  This means we can let the ANN learn whether to use batch normalization or not by itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad334c5e-427c-4cfe-a6bb-3b7c0a866e9f",
   "metadata": {},
   "source": [
    "### Step 3. Train the network\n",
    "\n",
    "We define a `train()` function to perform the training process, which takes the following parameters:\n",
    "- A data loader of the training set\n",
    "- A data loader of the validation set\n",
    "- A loss function (<span style=\"color:red\">use `nn.CrossEntropyLoss()` for multi-class classification</span>)\n",
    "- An optimizer (with L2 Regularization, learning rate decay, and early stopping)\n",
    "- The number of epoches to train\n",
    "\n",
    "There is one thing different for multi-class classification.\n",
    "- <span style=\"color:red\">The outputs of the ANN is logits</span>\n",
    "    - not probabilities\n",
    "    - not predictions\n",
    "- <span style=\"color:red\">The `nn.CrossEntropyLoss()` takes logits as inputs to compute the loss</span>\n",
    "    - `nn.CrossEntropyLoss()` applies softmax internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b10e2-ed3b-4e63-a516-b76a45d7ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function\n",
    "def train(train_dl, val_dl, model, loss_fn, optimizer, epochs, early_stopping_patience, lr_scheduler):\n",
    "    # initialization\n",
    "    min_val_loss = np.inf # initialize the val loss as an infinite positive value\n",
    "    patience_counter = 0 # set the initial patience counter to zero\n",
    "    train_batch_loss_history = [] # for recording the average train loss of a batch\n",
    "    train_epoch_loss_history = [] # for recording the average train loss of an epoch\n",
    "    val_batch_loss_history = [] # for recording the average val loss of an batch\n",
    "    val_epoch_loss_history = [] # for recording the average val loss of an epoch\n",
    "    \n",
    "    # start training\n",
    "    for epoch in range(epochs): # iterate pre-defined number of epoches\n",
    "        # train set\n",
    "        train_epoch_loss = 0.0 # initial train epoch loss is set to zero\n",
    "        model.train() # set the model in training mode\n",
    "        for (X, y) in train_dl: # get a batch of training samples\n",
    "            logits = model(X) # forward propagation\n",
    "            train_batch_loss = loss_fn(logits, y) # compute the current train batch loss\n",
    "            train_batch_loss.backward() # compute gradients by backpropagation\n",
    "            optimizer.step() # update learning parameters according to gradients\n",
    "            optimizer.zero_grad() # reset the gradients to zero\n",
    "            train_batch_loss_history.append(train_batch_loss.item()) # record current train batch loss\n",
    "            train_epoch_loss += train_batch_loss.item() # accumulate train batch losses for compute train epoch loss\n",
    "        train_epoch_loss /= len(train_dl) # compute current train epoch loss\n",
    "        train_epoch_loss_history.append(train_epoch_loss) # record current train epoch loss\n",
    "        # validation set\n",
    "        val_epoch_loss = 0.0 # initial val epoch loss is set to zero\n",
    "        model.eval() # set the model in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for (X, y) in val_dl: # get a batch of validation samples\n",
    "                logits = model(X) # forward propagation\n",
    "                val_batch_loss = loss_fn(logits, y) # compute the current val batch loss\n",
    "                val_batch_loss_history.append(val_batch_loss.item()) # record current val batch loss\n",
    "                val_epoch_loss += val_batch_loss.item() # accumulate val batch losses for compute val epoch loss\n",
    "            val_epoch_loss /= len(val_dl) # compute current val epoch loss\n",
    "            val_epoch_loss_history.append(val_epoch_loss) # record current val epoch loss\n",
    "        print(f\"Epoch {epoch + 1}: train loss = {train_epoch_loss:>5f}, val loss = {val_epoch_loss:>5f}\") # print log\n",
    "        # learning rate decay\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        lr_scheduler.step(val_epoch_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if old_lr != new_lr:\n",
    "            print(f'Learning rate reduced after epoch {epoch+1}')\n",
    "        # early stopping\n",
    "        if val_epoch_loss < min_val_loss:\n",
    "            min_val_loss = val_epoch_loss # update the new min val loss\n",
    "            patience_counter = 0 # reset patience counter to zero\n",
    "        else:\n",
    "            patience_counter += 1 # increase patience counter by 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "    return train_batch_loss_history, val_batch_loss_history, train_epoch_loss_history, val_epoch_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46109fc8-c4de-4050-9c79-62a945b8780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training hyper-parameters\n",
    "loss_fn = nn.CrossEntropyLoss() # use cross entropy loss for multi-class\n",
    "learning_rate = 1e-1\n",
    "weight_decay = 1e-5 # lambda for L2 regularization\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    "    weight_decay = weight_decay\n",
    ")\n",
    "epochs = 1000\n",
    "early_stopping_patience = 20\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode = 'min',\n",
    "    factor = 0.1, # each step multiply the learning rate with 0.1\n",
    "    patience = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a1ddd-a125-4a7c-a064-d7098de5b1b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "train_batch_loss_history, val_batch_loss_history, train_epoch_loss_history, val_epoch_loss_history = train(\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    early_stopping_patience,\n",
    "    lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c7881-58ae-46f2-a83c-5187299e2fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train loss history\n",
    "plt.figure()\n",
    "batch = np.arange(len(train_batch_loss_history))\n",
    "epoch = np.arange(len(train_epoch_loss_history))\n",
    "batches_per_epoch = (int(len(train_feature) / batch_size + 1))\n",
    "plt.plot(batch, train_batch_loss_history, '-', label = 'batch loss')\n",
    "plt.plot(epoch * batches_per_epoch, train_epoch_loss_history, '-', label = 'epoch loss')\n",
    "plt.title('train loss history')\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb14e8e-e0a1-4581-bc54-a2691d14eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the val loss history\n",
    "plt.figure()\n",
    "batch = np.arange(len(val_batch_loss_history))\n",
    "epoch = np.arange(len(val_epoch_loss_history))\n",
    "batches_per_epoch = (int(len(val_feature) / batch_size + 1))\n",
    "plt.plot(batch, val_batch_loss_history, '-', label = 'batch loss')\n",
    "plt.plot(epoch * batches_per_epoch, val_epoch_loss_history, '-', label = 'epoch loss')\n",
    "plt.title('validation loss history')\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de658078-2742-4c8d-b3c3-42b061cec303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train vs. val loss history\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(train_epoch_loss_history)), train_epoch_loss_history, '-', label = 'train loss')\n",
    "plt.plot(np.arange(len(val_epoch_loss_history)), val_epoch_loss_history, '-', label = 'val loss')\n",
    "plt.title('train vs. validation loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e876e0-5289-4974-8fbd-ba25164c7914",
   "metadata": {},
   "source": [
    "### Step 4. Save and load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f345a00-acdf-4958-a58b-87e6245b71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "file_name = 'multi_classification_net.pth'\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print('Saved PyTorch Model State to '+ file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152fcba-faef-4442-a9b5-2c89eac40135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = MultiClassificationNet(\n",
    "    n_features = feature.shape[1],\n",
    "    n_labels = encoded_label.shape[1]\n",
    ")\n",
    "model.load_state_dict(torch.load(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32ea2c-2fb8-49c6-b488-ba5d7fe88a17",
   "metadata": {},
   "source": [
    "### Step 5. Make predictions and evaluation\n",
    "\n",
    "Make predictions on the test set and evaluation the performance of the network.\n",
    "\n",
    "As the outputs of the ANN are logits, which can be used to calculate the loss using `nn.CrossEntropyLoss()`, but <span style=\"color:red\">we can't use these logits directly as the predicted probabilities or predicted classes</span>. We need to:\n",
    "- <span style=\"color:red\">Convert the logits to probabilities using `torch.softmax()`</span>\n",
    "- <span style=\"color:red\">Convert the probabilities to predicted classes using `torch.max()`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67b70ad-c11f-4a77-b741-a11b36d3b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to make predictions on test dataset and evaluate the performance\n",
    "def test(dataloader, model, loss_fn):\n",
    "    batch_logits_list = [] # for recording batch logits\n",
    "    batch_prob_list = [] # for recording batch probabilities\n",
    "    batch_pred_list = [] # for recording batch predictions\n",
    "    model.eval() # set the model in evaluation mode\n",
    "    with torch.no_grad(): # disable automatic gradient computing\n",
    "        loss = 0.0 # set initial test loss to zero\n",
    "        for (X, y) in dataloader: # get a batch from test samples\n",
    "            batch_logits = model(X) # forward propagation\n",
    "            batch_prob = torch.softmax(batch_logits, dim = -1) # convert logtis to probabilities\n",
    "            _, batch_pred = torch.max(batch_logits, 1) # conver probabilities to predictions\n",
    "            batch_loss = loss_fn(batch_logits, y) # compute current batch loss\n",
    "            loss += batch_loss.item() # accumulate batch losses for compute test loss\n",
    "            batch_logits_list.append(batch_logits) # record logits on current batch\n",
    "            batch_prob_list.append(batch_prob) # record probabilities on current batch\n",
    "            batch_pred_list.append(batch_pred) # record predictions on current batch\n",
    "        loss /= len(dataloader) # compute test loss\n",
    "        logits = np.concatenate(batch_logits_list) # reform the logits\n",
    "        prob = np.concatenate(batch_prob_list) # reform the probabilities\n",
    "        pred = np.concatenate(batch_pred_list) # reform the predictions\n",
    "        print(f\"test loss = {loss}\") # print log\n",
    "    return logits, prob, pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a6d40-dce5-4c49-b7d0-f34e7f8520b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on test set and evaluate the performance\n",
    "test_logits, test_prob, test_pred, test_loss = test(test_dl, model, loss_fn)\n",
    "print(classification_report(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45205b-cbfc-4327-84b5-fe77cc2261d4",
   "metadata": {},
   "source": [
    "## Part 2. Obtain an over-fitted model\n",
    "\n",
    "We have applied various techniques to prevent the model from overfitting:\n",
    "- L2 regularization / weight decay\n",
    "- early stopping\n",
    "- learning rate decay\n",
    "- Dropout\n",
    "- Batch normalization\n",
    "\n",
    "<span style=\"color:red\">Now let's try to remove or deactivate them and increase the capacity of the ANN to see if you can get an over-fitted model.</span>\n",
    "\n",
    "In practice, we always start with an over-fitted model to ensure it has enough capacity.\n",
    "\n",
    "Then we adopt different techniques to try to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
