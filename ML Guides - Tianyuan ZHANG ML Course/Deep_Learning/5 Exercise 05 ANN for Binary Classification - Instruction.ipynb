{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed377836-ce69-4a59-803c-dc027be754f2",
   "metadata": {},
   "source": [
    "# Exercise 05 ANN for Binary Classificatoin - Instruction\n",
    "\n",
    "## Pedagogy\n",
    "\n",
    "This notebook serves as an instruction for implementing ANNs using Pytorch to develop binary classification models.\n",
    "\n",
    "Please use this notebook as a reference and guide to complete the assignment.\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5dea63-0ae4-4b70-84e7-41fb8c6ea5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb04ceb-beef-4d64-891a-67e204b540da",
   "metadata": {},
   "source": [
    "## Part 1. Logistic Regression for binary classification\n",
    "\n",
    "### Step 1. Build the data pipeline\n",
    "\n",
    "We will use a toy dataset, the breast cancer wisconsin dataset, from `sklearn`. You can find more detials about this dataset at this [link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html).\n",
    "\n",
    "As the diabetes dataset is a tabular dataset that consists of numerical feature and target variables. We can use `torch.utils.data.TensorDataset`, a custom class that inherits from the `torch.utils.data.Dataset` class.\n",
    "\n",
    "In this step, we need to:\n",
    "1. Use `sklearn.datasets.load_breast_cancer()` to load the dataset\n",
    "2. Perform feature scaling for the numerical feature variables\n",
    "3. Divide the dataset into the training, <span style=\"color:red\">validation</span>, and test set\n",
    "    - We can still use `sklearn.model_selection.train_test_split()`\n",
    "    - First divide the entire dataset into the train+val set and the test set\n",
    "    - Then divide the train+val set into the train set and the val set \n",
    "5. Create `TensorDataset` instances to store the training, <span style=\"color:red\">validation</span>, and test set\n",
    "6. Create `DataLoader` instances to wrap the training, <span style=\"color:red\">validation</span>, and test set as iterable objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b07f6-d076-4841-b902-999514f0a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load brease cancer dataset\n",
    "feature, label = datasets.load_breast_cancer(\n",
    "    return_X_y = True,\n",
    "    as_frame = False, # get data as numpy array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879294c8-2180-44d2-8707-883f135580f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_feature = scaler.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0932e1b-cd30-4ca1-9aba-f670a3c921c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_val_feature, test_feature, train_val_label, test_label = train_test_split(\n",
    "    scaled_feature, label, test_size = 0.2, random_state = 0\n",
    ")\n",
    "train_feature, val_feature, train_label, val_label = train_test_split(\n",
    "    train_val_feature, train_val_label, test_size = 0.25, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac7a4d-99bd-4f3e-9c65-44a4461ca5b2",
   "metadata": {},
   "source": [
    "- 20% of data is used as the test set\n",
    "- 80%$\\times$25%=20% of data is used as the validation set\n",
    "- The rest 60% of data is used as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e17c42-5dd2-4aaf-b661-ea51243e848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train, validatoin, and test dataset\n",
    "# specify the type of data stored in the tensors to avoid incompatiblity\n",
    "train_ds = TensorDataset(\n",
    "    torch.tensor(train_feature, dtype = torch.float32),\n",
    "    torch.tensor(train_label, dtype = torch.float32)\n",
    ")\n",
    "val_ds = TensorDataset(\n",
    "    torch.tensor(val_feature, dtype = torch.float32),\n",
    "    torch.tensor(val_label, dtype = torch.float32)\n",
    ")\n",
    "test_ds = TensorDataset(\n",
    "    torch.tensor(test_feature, dtype = torch.float32),\n",
    "    torch.tensor(test_label, dtype = torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c94a0b-db8d-4524-ab19-345b3e7bea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and test data loaders\n",
    "batch_size = 64 # usually set to 2 to the nth power\n",
    "train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "val_dl = DataLoader(val_ds, batch_size = batch_size, shuffle = False)\n",
    "test_dl = DataLoader(test_ds, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e7f45-6a5d-4dcf-a6d7-238447ee85c7",
   "metadata": {},
   "source": [
    "### Step 2. Create the artificial neural network\n",
    "\n",
    "To define a neural network in `PyTorch`, we create a class that inherits from `torch.nn.Module`.\n",
    "\n",
    "Use an ANN as a logistic regression model means:\n",
    "- One-layer network\n",
    "    - The number of input neurons is equal to the number of input features\n",
    "    - Only one output neuron\n",
    "- <span style=\"color:red\">The activation function of the output neuron is `nn.Sigmoid()`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa083a2-ce73-4a32-a095-5aa7702f34d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom neural network class\n",
    "class LogisticRegressionNet(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6318c-8ee3-457c-b9ab-dfad39670bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the neural network\n",
    "model = LogisticRegressionNet(n_features = feature.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7517440-c786-4803-b9e6-302093c1f69c",
   "metadata": {},
   "source": [
    "### Step 3. Train the network\n",
    "\n",
    "We define a `train()` function to perform the training process, which takes the following parameters:\n",
    "- A data loader of the training set\n",
    "- <span style=\"color:red\">A data loader of the validation set</span>\n",
    "- A loss function (<span style=\"color:red\">use `nn.BCELoss()` for binary classification</span>)\n",
    "- An optimizer\n",
    "    - We also add <span style=\"color:red\">L2 Regularization</span> through the `weight_decay` parameter of the optimizer\n",
    "    - We implement <span style=\"color:red\">learning rate decay</span> by monitoring the loss on validation set\n",
    "    - We implement <span style=\"color:red\">early stopping</span> by monitoring the loss on validation set\n",
    "- The number of epoches to train\n",
    "\n",
    "In a single training loop, we need to:\n",
    "1. Make predictions on the training set (in batches)\n",
    "2. Compute the loss on the training set (in batches)\n",
    "3. Update network parameters according to gradients (in batches)\n",
    "4. <span style=\"color:red\">Make predictions on the validation set (in batches)</span>\n",
    "5. <span style=\"color:red\">Compute the loss on the validation set (in batches)</span>\n",
    "6. <span style=\"color:red\">Monitor the changes in the validation loss (in epoch), to:</span>\n",
    "    - <span style=\"color:red\">Perform leanring rate decay</span>\n",
    "    - <span style=\"color:red\">Perform early stopping</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750402f3-07b4-419c-a3b7-e7032903a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training function\n",
    "def train(train_dl, val_dl, model, loss_fn, optimizer, epochs, early_stopping_patience, lr_scheduler):\n",
    "    # initialization\n",
    "    min_val_loss = np.inf # initialize the val loss as an infinite positive value\n",
    "    patience_counter = 0 # set the initial patience counter to zero\n",
    "    train_batch_loss_history = [] # for recording the average train loss of a batch\n",
    "    train_epoch_loss_history = [] # for recording the average train loss of an epoch\n",
    "    val_batch_loss_history = [] # for recording the average val loss of an batch\n",
    "    val_epoch_loss_history = [] # for recording the average val loss of an epoch\n",
    "    \n",
    "    # start training\n",
    "    for epoch in range(epochs): # iterate pre-defined number of epoches\n",
    "        # train set\n",
    "        train_epoch_loss = 0.0 # initial train epoch loss is set to zero\n",
    "        model.train() # set the model in training mode\n",
    "        for (X, y) in train_dl: # get a batch of training samples\n",
    "            prob = model(X).squeeze() # make predictions, squeeze() reduce `prob` to 1D tensor\n",
    "            train_batch_loss = loss_fn(prob, y) # compute the current train batch loss\n",
    "            train_batch_loss.backward() # compute gradients by backpropagation\n",
    "            optimizer.step() # update learning parameters according to gradients\n",
    "            optimizer.zero_grad() # reset the gradients to zero\n",
    "            train_batch_loss_history.append(train_batch_loss.item()) # record current train batch loss\n",
    "            train_epoch_loss += train_batch_loss.item() # accumulate train batch losses for compute train epoch loss\n",
    "        train_epoch_loss /= len(train_dl) # compute current train epoch loss\n",
    "        train_epoch_loss_history.append(train_epoch_loss) # record current train epoch loss\n",
    "        # validation set\n",
    "        val_epoch_loss = 0.0 # initial val epoch loss is set to zero\n",
    "        model.eval() # set the model in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for (X, y) in val_dl: # get a batch of validation samples\n",
    "                prob = model(X).squeeze() # make predictions, squeeze() reduce `prob` to 1D tensor\n",
    "                val_batch_loss = loss_fn(prob, y) # compute the current val batch loss\n",
    "                val_batch_loss_history.append(val_batch_loss.item()) # record current val batch loss\n",
    "                val_epoch_loss += val_batch_loss.item() # accumulate val batch losses for compute val epoch loss\n",
    "            val_epoch_loss /= len(val_dl) # compute current val epoch loss\n",
    "            val_epoch_loss_history.append(val_epoch_loss) # record current val epoch loss\n",
    "        print(f\"Epoch {epoch + 1}: train loss = {train_epoch_loss:>5f}, val loss = {val_epoch_loss:>5f}\") # print log\n",
    "        # learning rate decay\n",
    "        old_lr = optimizer.param_groups[0]['lr']\n",
    "        lr_scheduler.step(val_epoch_loss)\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        if old_lr != new_lr:\n",
    "            print(f'Learning rate reduced after epoch {epoch+1}')\n",
    "        # early stopping\n",
    "        if val_epoch_loss < min_val_loss:\n",
    "            min_val_loss = val_epoch_loss # update the new min val loss\n",
    "            patience_counter = 0 # reset patience counter to zero\n",
    "        else:\n",
    "            patience_counter += 1 # increase patience counter by 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "    return train_batch_loss_history, val_batch_loss_history, train_epoch_loss_history, val_epoch_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35101aeb-ff0f-4585-97f4-b53a0a29eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training hyper-parameters\n",
    "loss_fn = nn.BCELoss() # binary cross entropy loss\n",
    "learning_rate = 1e-1\n",
    "weight_decay = 1e-5 # lambda for L2 regularization\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = learning_rate,\n",
    "    weight_decay = weight_decay\n",
    ")\n",
    "epochs = 1000\n",
    "early_stopping_patience = 20\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode = 'min',\n",
    "    factor = 0.1, # each step multiply the learning rate with 0.1\n",
    "    patience = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4a1cb-bdca-4623-b598-ccc4a732a094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the neural network\n",
    "train_batch_loss_history, val_batch_loss_history, train_epoch_loss_history, val_epoch_loss_history = train(\n",
    "    train_dl,\n",
    "    val_dl,\n",
    "    model,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    early_stopping_patience,\n",
    "    lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47aee2-1258-48ca-befa-4b4bdc1611f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train loss history\n",
    "plt.figure()\n",
    "batch = np.arange(len(train_batch_loss_history))\n",
    "epoch = np.arange(len(train_epoch_loss_history))\n",
    "batches_per_epoch = (int(len(train_feature) / batch_size + 1))\n",
    "plt.plot(batch, train_batch_loss_history, '-', label = 'batch loss')\n",
    "plt.plot(epoch * batches_per_epoch, train_epoch_loss_history, '-', label = 'epoch loss')\n",
    "plt.title('train loss history')\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10e217-84c4-433b-b727-32337de09872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the val loss history\n",
    "plt.figure()\n",
    "batch = np.arange(len(val_batch_loss_history))\n",
    "epoch = np.arange(len(val_epoch_loss_history))\n",
    "batches_per_epoch = (int(len(val_feature) / batch_size + 1))\n",
    "plt.plot(batch, val_batch_loss_history, '-', label = 'batch loss')\n",
    "plt.plot(epoch * batches_per_epoch, val_epoch_loss_history, '-', label = 'epoch loss')\n",
    "plt.title('validation loss history')\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99275f54-9f55-4481-9908-9ce2ab150401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train vs. val loss history\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(train_epoch_loss_history)), train_epoch_loss_history, '-', label = 'train loss')\n",
    "plt.plot(np.arange(len(val_epoch_loss_history)), val_epoch_loss_history, '-', label = 'val loss')\n",
    "plt.title('train vs. validation loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08926ae-86a7-4084-b0f0-b78a3e502e58",
   "metadata": {},
   "source": [
    "### Step 4. Save and load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7a5b9-f0f2-41ee-8cf5-f65c9ac39578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "file_name = 'logistic_regression_net.pth'\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print('Saved PyTorch Model State to '+ file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4dbfea-d7d6-4618-a30f-c3ea93810f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = LogisticRegressionNet(n_features = feature.shape[1])\n",
    "model.load_state_dict(torch.load(file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b625e70-fe16-4d9d-b0f2-48af95c7720e",
   "metadata": {},
   "source": [
    "### Step 5. Make predictions and evaluation\n",
    "\n",
    "Make predictions on the test set and evaluation the performance of the network.\n",
    "\n",
    "To do that in batch, we can define a `test()` function, which is similar to the `train` function but much simpler.\n",
    "\n",
    "Note that, <span style=\"color:red\">the output of the logistic regression network is the probabilities belongs to the positive class. We need to convert the probabilities to categorical predictions by a threshold.</span> Then we can use the classification metrics to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f9bb9-e836-44ec-a014-3a170e48e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to make predictions on test dataset and evaluate the performance\n",
    "def test(dataloader, model, loss_fn):\n",
    "    batch_prob_list = [] # for recording batch predictions\n",
    "    model.eval() # set the model in evaluation mode\n",
    "    with torch.no_grad(): # disable automatic gradient computing\n",
    "        loss = 0.0 # set initial test loss to zero\n",
    "        for (X, y) in dataloader: # get a batch from test samples\n",
    "            batch_prob = model(X).squeeze() # make predictions, squeeze() reduce `batch_prob` to 1D tensor\n",
    "            batch_loss = loss_fn(batch_prob, y) # compute current batch loss\n",
    "            loss += batch_loss.item() # accumulate batch losses for compute test loss\n",
    "            batch_prob_list.append(batch_prob) # record predictions on current batch\n",
    "        loss /= len(dataloader) # compute test loss\n",
    "        prob = np.concatenate(batch_prob_list) # reform the predictions as a numpy 1D array\n",
    "        print(f\"test loss = {loss}\") # print log\n",
    "    return prob, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23792c29-e827-45ca-a523-369a2910f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction on test set and evaluate the performance\n",
    "test_prob, test_loss = test(test_dl, model, loss_fn)\n",
    "test_pred = (test_prob > 0.5) # convert the probability to predicted class by a threshold\n",
    "print(classification_report(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ab1fb-6849-4097-bc57-db6d01a8ff9d",
   "metadata": {},
   "source": [
    "## Part 2. Multi-layer ANN for binary classification\n",
    "\n",
    "Instead of using an one-layer ANN that behaves like a logistic regression model, we can also define a multi-layer ANN as a binary classifer with higher capacity.\n",
    "\n",
    "For example, we can define a deeper and more complex ANN as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55517264-53a1-4f66-a912-ee03cb07f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom neural network class\n",
    "class BinaryClassificationNet(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b683b6-ceb4-4de7-b490-44958241d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the neural network\n",
    "model = BinaryClassificationNet(n_features = feature.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796fa4c-7bb1-4016-b7e2-ebeabd5df170",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Now let's train a binary classifier using the above defined neural network structure to see if you can improve the performance.</span>\n",
    "- You can change the following options to see the effects on the training process:\n",
    "    - batch size\n",
    "    - number of layers and neurons\n",
    "    - activation functions\n",
    "    - lambda of L2 regularization\n",
    "    - early stopping patience\n",
    "    - learning rate decay patience"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
