{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747fc603",
   "metadata": {},
   "source": [
    "# Exercise 02 Ensemble Learning II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4dac9",
   "metadata": {},
   "source": [
    "## Pedagogy\n",
    "\n",
    "This notebook contains both theoretical explanations and executable cells to execute your code.\n",
    "\n",
    "When you see the <span style=\"color:red\">**[TBC]**</span> (To Be Completed) sign, it means that you need to perform an action else besides executing the cells of code that already exist. These actions can be:\n",
    "- Complete the code with proper comments\n",
    "- Respond to a question\n",
    "- Write an analysis\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f50db",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a01858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries used in this notebook here\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import classification_report, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee517054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04990cf7",
   "metadata": {},
   "source": [
    "## Part 1. AdaBoost Classifier\n",
    "\n",
    "In this part, we will using AdaBoost algorithm to build a classifier with a toy dataset.\n",
    "\n",
    "We will ececute the following steps:\n",
    "\n",
    "- Load and explore dataset\n",
    "- Train test split\n",
    "- Build an AdaBoost classifier with default hyper-parameters\n",
    "- Evaluation the classifier using the test dataset\n",
    "- Obtain the byproduct feature importance\n",
    "- Test the effects of hyper-parameters on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3b220",
   "metadata": {},
   "source": [
    "### 1.1 Load dataset\n",
    "\n",
    "We will use a toy dataset, the [wine recognition dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset), provided by `scikit-learn`.\n",
    "\n",
    "There are 13 feature variables in the dataset, which are the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators.\n",
    "\n",
    "There are one target variable, with 3 unique categories, representing the 3 different cultivators.\n",
    "\n",
    "There are 178 instances in the dataset.\n",
    "\n",
    "We will use this dataset to build a multi-class classifier with AdaBoost.\n",
    "\n",
    "Use `sklearn.dataset.load_wine()` to get this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a87db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "feature_df, target_df = datasets.load_wine(\n",
    "    return_X_y = True, # If True, returns (data.data, data.target) instead of a Bunch object.\n",
    "    as_frame = True # If True, the data is a pandas DataFrame including columns with appropriate dtypes (numeric).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first five rows of the features\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the unique values of the target variable\n",
    "target_df.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f2bb0",
   "metadata": {},
   "source": [
    "### 1.2 Train test split\n",
    "\n",
    "We will split the whole dataset into two parts: the training and test dataset.\n",
    "- 70% for training\n",
    "- 30% for test\n",
    "\n",
    "Use `sklearn.model_selection.train_test_split()` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_df.values, # call `.values` to convert the feature from pd.DataFrame to np.array\n",
    "    target_df.values, # ca;; `.values` to convert the target from pd.Series to np.array\n",
    "    train_size = 0.7, # 70% for training, 30% for test\n",
    "    random_state = 0 # controls the shuffling, set to zero for reproduciblillity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f97b7",
   "metadata": {},
   "source": [
    "### 1.3 Build a classifier using AdaBoost\n",
    "\n",
    "Build a multi-class classifier using AdaBoost with default hyper-parameters.\n",
    "\n",
    "- `estimator = DecisionTreeClassifier(max_depth = 1)`\n",
    "- `n_estimators = 50`\n",
    "- `learning_rate = 1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the AdaBoost classifier with default hyper-parameters\n",
    "clf = AdaBoostClassifier(\n",
    "    random_state = 0 # set random state to 0 for reproduciblity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c277a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training dataset\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243d93e",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation using the test dataset\n",
    "\n",
    "Evaluate the performance of the classifier using the test dataset.\n",
    "\n",
    "Use `sklearn.metrics.classification_report()` to get the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict categories for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95eec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain classification metrics using `classification_report`\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5103ef4",
   "metadata": {},
   "source": [
    "### 1.6 Obtain feature importances\n",
    "\n",
    "Feature importance is a byproduct provided by AdaBoost algorithm when the weak learner is tree-based.\n",
    "\n",
    "The feature importance is the weighted average of feature importance across all trees in the ensemble.\n",
    "\n",
    "The weights here refer to the weights of different learners, which are the same as the weights to aggregate the base predictions into the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain feature importances\n",
    "feature_importances = pd.Series(\n",
    "    data = clf.feature_importances_,\n",
    "    index = feature_df.columns\n",
    ")\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b0c4e",
   "metadata": {},
   "source": [
    "We can find, unlike random forests that most of the features have importance greater than 0, the feature importance from AdaBoost has a lot of 0 values.\n",
    "\n",
    "This is because the default weak learner in AdaBoost is a decision stump (a decision tree with depth = 1).\n",
    "\n",
    "It means, each weak learner only uses one feature. And there are a lot of features that are never used.\n",
    "\n",
    "So AdaBoost only use a limited number of features to achieve good performance. When the number of features is limited, consider to use AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f79d6",
   "metadata": {},
   "source": [
    "### 1.7 Test with other hyper-parameter values\n",
    "\n",
    "The three key hyper-parameters will affect the performance of the ensemble.\n",
    "\n",
    "Let's test these effects by varying one hyper-parameter and keeping the rest unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the effect of n_estimators on performance\n",
    "n_estimators = [2, 5, 10, 25, 50, 75, 100]\n",
    "f1_weighted = []\n",
    "for item in n_estimators:\n",
    "    clf = AdaBoostClassifier(\n",
    "        n_estimators = item,\n",
    "        random_state = 0\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_weighted.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_estimators, f1_weighted, '.-')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('f1_weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bf9ca",
   "metadata": {},
   "source": [
    "- If `n_estimators` is too low\n",
    "    - There won't be enough weak learners in the ensemble to correct the high bias.\n",
    "    - The ensemble might be to simple and underfit the data.\n",
    "- If `n_estimators` is too high\n",
    "    - Increase `n_estimator` have a diminishing effect on performance\n",
    "    - Too many weak learners might lead to over-fitting, which down-grades the performance\n",
    "    - More estimators increase the training time and require more computational resource\n",
    "- We can select an optimal value of `n_estimators` through hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8dcaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the effect of learning_rate on performance\n",
    "learning_rate = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "f1_weighted = []\n",
    "for item in learning_rate:\n",
    "    clf = AdaBoostClassifier(\n",
    "        learning_rate = item,\n",
    "        random_state = 0\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_weighted.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(learning_rate, f1_weighted, '.-')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('f1_weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2de7b",
   "metadata": {},
   "source": [
    "- If `learning_rate` is too low\n",
    "    - A low `learning_rate` makes the ensemble more conservative, requiring more estimators to achieve good performance.\n",
    "    - In general, a low `learning_rate` with enough weak learners can lead to a robust ensemble with good performance.\n",
    "- If `learning_rate` is too high\n",
    "    - A high `learning_rate` allows each weak learner to contribute more to the final ensemble.\n",
    "    - This can speed up training process and reduce the number of required weak learners.\n",
    "    - But it also increase the risk of over-fitting, which might not be able to obtain the best performance.\n",
    "- We can make the trade-off between `learning_rate` and `n_estimators` through hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the effect of the complexity of base estimators on performance\n",
    "max_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "f1_weighted = []\n",
    "for item in max_depth:\n",
    "    clf = AdaBoostClassifier(\n",
    "        estimator = DecisionTreeClassifier(max_depth = item),\n",
    "        random_state = 0\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_weighted.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(max_depth, f1_weighted, '.-')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('f1_weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52904575",
   "metadata": {},
   "source": [
    "- If `max_depth` of the base tree is too low\n",
    "    - The base tree might be too simple, and the resulting ensemble is also too simple for complex problems\n",
    "- If `max_depth` of the base tree is too high\n",
    "    - The base tree is too complex with low bias and high variance\n",
    "    - Cannot meet the prerequistes of Boosting method\n",
    "    - Might be over-fitted and the performance can be down-graded\n",
    "- We should select a low `max_depth` to keep the base tree shallow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf1206",
   "metadata": {},
   "source": [
    "## Part 2. Gradient Boosting Classifier\n",
    "\n",
    "In this part, we will using Gradient Boosting algorithm to solve the same problem.\n",
    "\n",
    "We will ececute the following steps:\n",
    "\n",
    "- Build a Gradient Boosting classifier with default hyper-parameters\n",
    "    - Implement early stopping or not\n",
    "- Obtain the byproduct feature importance\n",
    "- Test the effects of hyper-parameters on performance\n",
    "- Hyper-parameter tuning through cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c20a8c",
   "metadata": {},
   "source": [
    "### 2.1 Build and evaluate a classifier using Gradient Boosting\n",
    "\n",
    "Build and evaluate a multi-class classifier using Gradient Boosting with default hyper-parameters.\n",
    "\n",
    "- `learning_rate = 0.1`\n",
    "- `n_estimators = 100`\n",
    "- `max_depth = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1e9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the AdaBoost classifier with default hyper-parameters\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_iter_no_change = 5, # set to None to unable early stopping\n",
    "    random_state = 0, # set random state to 0 for reproduciblity\n",
    "    verbose = 3 # set the verbose level for printing progress and performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11138c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model to the training dataset\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict categories for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7010fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain classification metrics using `classification_report`\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0684ea9",
   "metadata": {},
   "source": [
    "### 2.2 Obtain feature importances\n",
    "\n",
    "As `GradientBoostingClassifier()` is also tree-based, we can obtain the feature importance as a by product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be33979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain feature importances\n",
    "feature_importances = pd.Series(\n",
    "    data = clf.feature_importances_,\n",
    "    index = feature_df.columns\n",
    ")\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f60c03",
   "metadata": {},
   "source": [
    "We can see there is no zero value in the feature importance.\n",
    "\n",
    "This is because the default depth of the base tree in `GradientBoostingClassifier()` is 3, thus more features involved than using the decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f2c6c",
   "metadata": {},
   "source": [
    "### 2.3 Test with other hyper-parameter values\n",
    "\n",
    "The three key hyper-parameters will affect the performance of the ensemble.\n",
    "\n",
    "Let's test these effects by varying one hyper-parameter and keeping the rest unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the effect of n_estimators on performance\n",
    "n_estimators = [2, 5, 10, 25, 50, 75, 100]\n",
    "f1_weighted = []\n",
    "for item in n_estimators:\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators = item,\n",
    "        random_state = 0\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_weighted.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(n_estimators, f1_weighted, '.-')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('f1_weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c017e",
   "metadata": {},
   "source": [
    "- If `n_estimators` is too low\n",
    "    - There won't be enough weak learners in the ensemble to correct the remaining residuals.\n",
    "    - The ensemble might be to simple and underfit the data.\n",
    "- If `n_estimators` is too high\n",
    "    - Too many weak learners might lead to over-fitting, which down-grades the performance\n",
    "    - We can use early stopping to prevent the ensemble from over-fitting\n",
    "- We can select an optimal value of `n_estimators` through hyper-parameter tuning\n",
    "- Or we can set `n_estimators` to a large value and adopt early stopping at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eea388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the effect of learning_rate on performance\n",
    "learning_rate = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "f1_weighted = []\n",
    "for item in learning_rate:\n",
    "    clf = GradientBoostingClassifier(\n",
    "        learning_rate = item,\n",
    "        random_state = 0\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_weighted.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(learning_rate, f1_weighted, '.-')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('f1_weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d809d",
   "metadata": {},
   "source": [
    "- If `learning_rate` is too low\n",
    "    - A low `learning_rate` makes the ensemble more conservative, requiring more estimators to achieve good performance.\n",
    "    - In general, a low `learning_rate` with enough weak learners can lead to a robust ensemble with good performance.\n",
    "- If `learning_rate` is too high\n",
    "    - A high `learning_rate` allows each weak learner to contribute more to the final ensemble.\n",
    "    - This can speed up training process and reduce the number of required weak learners.\n",
    "    - But it also increase the risk of over-fitting, which might not be able to obtain the best performance.\n",
    "- We can find an optimal `learning_rate` through hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabcdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the effect of the complexity of base estimators on performance\n",
    "max_depth = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "f1_weighted = []\n",
    "for item in max_depth:\n",
    "    clf = GradientBoostingClassifier(\n",
    "        max_depth = item,\n",
    "        random_state = 0\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    f1_weighted.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(max_depth, f1_weighted, '.-')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('f1_weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b326f6",
   "metadata": {},
   "source": [
    "- If `max_depth` of the base tree is too low\n",
    "    - The base tree might be too simple, and the resulting ensemble is also too simple for complex problems\n",
    "- If `max_depth` of the base tree is too high\n",
    "    - The base tree is too complex, and the resulting ensemble is also too complex and over-fitted\n",
    "- Gradient Boosting doesn't require the base tree to be shallow\n",
    "- We need to select a proper `max_depth` according to the complexity of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0860907",
   "metadata": {},
   "source": [
    "### 2.4 Hyper-parameter tuning through cross-validation\n",
    "\n",
    "Considering the interaction between `n_estimators`, `learning_rate`, `max_depth`, and whether to adopt early stopping or not, we can use hyper-parameter tuning to find the best combination of the hyper-parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hyper-parameters to search\n",
    "param_dict = {\n",
    "    'learning_rate': [1e-2, 1e-1, 1.0, 1e1, 1e2],\n",
    "    'n_estimators': [10, 50, 100, 200, 500],\n",
    "    'max_depth': [1, 3, 5, 7, 9, None],\n",
    "    'n_iter_no_change': [None, 1, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter tuning through cross-validation\n",
    "grid_clf = GridSearchCV(\n",
    "    estimator = GradientBoostingClassifier(random_state = 0),\n",
    "    param_grid = param_dict,\n",
    "    scoring = 'f1_weighted',\n",
    "    refit = True,\n",
    "    cv = 5,\n",
    "    verbose = 1,\n",
    "    n_jobs = -1\n",
    ")\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the best hyper-parameters and the best score\n",
    "print('Best hyper-parameters:', grid_clf.best_params_)\n",
    "print('Best score:', grid_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict categories for test dataset\n",
    "y_pred = grid_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ce264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain classification metrics using `classification_report`\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f91d3",
   "metadata": {},
   "source": [
    "## Part 3. Stacking Classifier\n",
    "\n",
    "In this part, we will using Stacking algorithm to solve the same problem.\n",
    "\n",
    "We will ececute the following steps:\n",
    "\n",
    "- Declare the base estimators\n",
    "- Train the stacking classifer\n",
    "- Evaluate the stacking classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9798d",
   "metadata": {},
   "source": [
    "### 3.1 Declare base estimators\n",
    "\n",
    "To be noted, the base estimators can be built using different algorithms, or the same algorithm with different hyper-parameters.\n",
    "\n",
    "For some algorithms that require the specific data pre-processing steps, like feature scaling for KNN and SVM, don't forget to embed the pre-processing steps as a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare a list of base estimators to be stacked together\n",
    "estimators = [\n",
    "    ('decision tree', DecisionTreeClassifier(\n",
    "        max_depth = 5,\n",
    "        random_state = 0\n",
    "    )),\n",
    "    ('KNN', Pipeline([\n",
    "        ('standard scaler', StandardScaler()),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ])),\n",
    "    ('SVC', Pipeline([\n",
    "        ('standard scaler', StandardScaler()),\n",
    "        ('svc', SVC())\n",
    "    ]))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4509ebc",
   "metadata": {},
   "source": [
    "### 3.2 Build the stacked classifier\n",
    "\n",
    "Here, we use the default algorithm (logistic regression) as the meta learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e130a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stacked classifier with logistic regression as the final estimator\n",
    "clf = StackingClassifier(\n",
    "    estimators = estimators,\n",
    "    final_estimator = LogisticRegression(),\n",
    "    n_jobs = -1,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training dataset\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959d4e7",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cedb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict categories for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbf804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain classification metrics using `classification_report`\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f0a77e",
   "metadata": {},
   "source": [
    "## Part 4. Hands-on exercise\n",
    "\n",
    "<span style=\"color:red\">**This exercise is an assignment, the submission deadline on Learn of this assignment is 25/03/2024 23:59.**</span>\n",
    "\n",
    "In this exercise, you are required to build a regression model using the three ensemble learning methods we've learned today.\n",
    "\n",
    "The problem to be solved is predicting the price of flights.\n",
    "\n",
    "Please download the flight price dataset from Learn.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Load and pre-process the dataset\n",
    "- Build and evaluate a regression model using:\n",
    "    - AdaBoost\n",
    "    - Gradient Boosting\n",
    "    - Stacking\n",
    "\n",
    "<span style=\"color:red\">**Warning**</span>: Be aware of the size of the dataset, make sure:\n",
    "\n",
    "- The scripts are executable on your device (whether your computer or Google Colab)\n",
    "- The submitted jupyter notebook has been already executed and contains all the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cd016",
   "metadata": {},
   "source": [
    "### Task 1. Load and pre-process the dataset\n",
    "\n",
    "You need to load the dataset and perform necessary pre-processing steps.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Load the dataset\n",
    "- Encode categorical features\n",
    "- Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd31243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# load the dataset\n",
    "# hint: pandas.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49077e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Encode categorical features\n",
    "# hint: sklearn.preprocessing.LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a096dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Train test split\n",
    "# hint: sklearn.model_selection.train_test_split()\n",
    "# hint: first divide the encoded dataset into features and target, then perform train test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2195bca",
   "metadata": {},
   "source": [
    "### Task 2. AdaBoost Regressor\n",
    "\n",
    "You need to build and evaluate a regression model using AdaBoost algorithm.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Hyper-parameter-tuning through cross validation\n",
    "- Evaluate the performance on test dataset\n",
    "    - Calculate RMSE and R2 score\n",
    "    - Visualize the prediction results of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d12957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# AdaBoost Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71159d",
   "metadata": {},
   "source": [
    "### Task 3. Gradient Boosting Regressor\n",
    "\n",
    "You need to build and evaluate a regression model using Gradient Boosting algorithm.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Hyper-parameter-tuning through cross validation\n",
    "- Evaluate the performance on test dataset\n",
    "    - Calculate RMSE and R2 score\n",
    "    - Visualize the prediction results of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5e5a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Gradient Boosting Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0077ba",
   "metadata": {},
   "source": [
    "### Task 4. Stacking Regressor\n",
    "\n",
    "You need to build and evaluate a regression model using Stacking algorithm.\n",
    "\n",
    "<span style=\"color:red\">**[TBC]**</span> Please complete the following tasks:\n",
    "\n",
    "- Train a stacking regressor on training dataset\n",
    "    - Select a list of base learner\n",
    "        - Choose proper regression algorithms\n",
    "        - If the algorithm needs specific pre-processing steps, embed the steps as a pipeline\n",
    "    - Select the meta-learner\n",
    "        - Choose proper regression algorithm\n",
    "- Evaluate the performance on test dataset\n",
    "    - Calculate RMSE and R2 score\n",
    "    - Visualize the prediction results of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TBC] complete your code here with proper comments\n",
    "# Stacking Regressor\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
