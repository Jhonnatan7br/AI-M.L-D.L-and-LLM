{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4144c129",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# 2.5 Automatic Differentiation\n",
    "\n",
    "Calculating derivatives is the crucial step\n",
    "in all the optimization algorithms\n",
    "that we will use to train deep networks.\n",
    "While the calculations are straightforward,\n",
    "working them out by hand can be tedious and error-prone, \n",
    "and these issues only grow\n",
    "as our models become more complex.\n",
    "\n",
    "Fortunately all modern deep learning frameworks\n",
    "take this work off our plates\n",
    "by offering *automatic differentiation*\n",
    "(often shortened to *autograd*). \n",
    "As we pass data through each successive function,\n",
    "the framework builds a *computational graph* \n",
    "that tracks how each value depends on others.\n",
    "To calculate derivatives, \n",
    "automatic differentiation \n",
    "works backwards through this graph\n",
    "applying the chain rule. \n",
    "The computational algorithm for applying the chain rule\n",
    "in this fashion is called *backpropagation*.\n",
    "\n",
    "While autograd libraries have become\n",
    "a hot concern over the past decade,\n",
    "they have a long history. \n",
    "In fact the earliest references to autograd\n",
    "date back over half of a century.\n",
    "The core ideas behind modern backpropagation\n",
    "date to a PhD thesis from 1980 \n",
    "and were further developed in the late 1980s.\n",
    "While backpropagation has become the default method \n",
    "for computing gradients, it is not the only option. \n",
    "For instance, the Julia programming language employs \n",
    "forward propagation. \n",
    "Before exploring methods, \n",
    "let's first master the autograd package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130439cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:08.286501Z",
     "iopub.status.busy": "2023-08-18T19:26:08.285693Z",
     "iopub.status.idle": "2023-08-18T19:26:10.052257Z",
     "shell.execute_reply": "2023-08-18T19:26:10.050994Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab3850",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 2.5.1 A Simple Function\n",
    "\n",
    "Let's assume that we are interested\n",
    "in **differentiating the function\n",
    "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "with respect to the column vector $\\mathbf{x}$.**\n",
    "To start, we assign `x` an initial value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4253cfab",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.056833Z",
     "iopub.status.busy": "2023-08-18T19:26:10.055871Z",
     "iopub.status.idle": "2023-08-18T19:26:10.084858Z",
     "shell.execute_reply": "2023-08-18T19:26:10.083727Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75614b0",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "**Before we calculate the gradient\n",
    "of $y$ with respect to $\\mathbf{x}$,\n",
    "we need a place to store it.**\n",
    "In general, we avoid allocating new memory\n",
    "every time we take a derivative \n",
    "because deep learning requires \n",
    "successively computing derivatives\n",
    "with respect to the same parameters\n",
    "a great many times,\n",
    "and we might risk running out of memory.\n",
    "Note that the gradient of a scalar-valued function\n",
    "with respect to a vector $\\mathbf{x}$\n",
    "is vector-valued with \n",
    "the same shape as $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a001d1e",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.088716Z",
     "iopub.status.busy": "2023-08-18T19:26:10.087816Z",
     "iopub.status.idle": "2023-08-18T19:26:10.092878Z",
     "shell.execute_reply": "2023-08-18T19:26:10.091740Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# Can also create x = torch.arange(4.0, requires_grad=True)\n",
    "x.requires_grad_(True)\n",
    "x.grad  # The gradient is None by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74bc02",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "**We now calculate our function of `x` and assign the result to `y`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3bd777",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.096336Z",
     "iopub.status.busy": "2023-08-18T19:26:10.095772Z",
     "iopub.status.idle": "2023-08-18T19:26:10.105236Z",
     "shell.execute_reply": "2023-08-18T19:26:10.104075Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3067490",
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "**We can now take the gradient of `y`\n",
    "with respect to `x`** by calling \n",
    "its `backward` method.\n",
    "Next, we can access the gradient \n",
    "via `x`'s `grad` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b134ae",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.108600Z",
     "iopub.status.busy": "2023-08-18T19:26:10.108011Z",
     "iopub.status.idle": "2023-08-18T19:26:10.160854Z",
     "shell.execute_reply": "2023-08-18T19:26:10.159702Z"
    },
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1390b",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "**We already know that the gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
    "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.**\n",
    "We can now verify that the automatic gradient computation\n",
    "and the expected result are identical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5030e37d",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.164665Z",
     "iopub.status.busy": "2023-08-18T19:26:10.163930Z",
     "iopub.status.idle": "2023-08-18T19:26:10.171033Z",
     "shell.execute_reply": "2023-08-18T19:26:10.169923Z"
    },
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da440e48",
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "**Now let's calculate \n",
    "another function of `x`\n",
    "and take its gradient.**\n",
    "Note that PyTorch does not automatically \n",
    "reset the gradient buffer \n",
    "when we record a new gradient. \n",
    "Instead, the new gradient\n",
    "is added to the already-stored gradient.\n",
    "This behavior comes in handy\n",
    "when we want to optimize the sum \n",
    "of multiple objective functions.\n",
    "To reset the gradient buffer,\n",
    "we can call `x.grad.zero_()` as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "add5cf4b",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    },
    "execution": {
     "iopub.execute_input": "2023-08-18T19:26:10.174691Z",
     "iopub.status.busy": "2023-08-18T19:26:10.173957Z",
     "iopub.status.idle": "2023-08-18T19:26:10.181847Z",
     "shell.execute_reply": "2023-08-18T19:26:10.180759Z"
    },
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()  # Reset the gradient\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
